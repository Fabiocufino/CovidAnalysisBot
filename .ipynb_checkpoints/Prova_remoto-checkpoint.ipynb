{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dfe230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basilicata\n",
      "Abruzzo\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This module contains all IT-specific data loading and data cleaning routines.\n",
    "\"\"\"\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "\n",
    "def get_raw_covid_data_it():\n",
    "    url = \"https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-regioni/dpc-covid19-ita-regioni.csv\"\n",
    "    #url = \"../git_dpc/COVID-19/dati-regioni/dpc-covid19-ita-regioni.csv\"\n",
    "    data = pd.read_csv(url)\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_covid_data_it(data: pd.DataFrame, run_date: pd.Timestamp):\n",
    "    data = data.rename(columns={\"denominazione_regione\": \"region\"})\n",
    "    data = data.rename(columns={\"totale_casi\": \"positive\"})\n",
    "    data = data.rename(columns={\"tamponi\": \"total\"})\n",
    "    data[\"date\"] = pd.to_datetime(data[\"data\"], format=\"%Y-%m-%d\").dt.date\n",
    "    data = data.set_index([\"region\", \"date\"]).sort_index()\n",
    "    data = data[[\"positive\", \"total\"]]\n",
    "    data = data.astype(float)\n",
    "    \n",
    "    # Data Clean\n",
    "    data.loc[idx[\"Abruzzo\", pd.Timestamp(\"2020-07-26\")], \"total\"] = 124891\n",
    "    data.loc[idx[\"Abruzzo\", pd.Timestamp(\"2020-06-19\")], \"positive\"] = 3281\n",
    "    data.loc[idx[\"Abruzzo\", pd.Timestamp(\"2020-07-22\")], \"positive\"] = 3344\n",
    "    \n",
    "    data.loc[idx[\"Calabria\", pd.Timestamp(\"2020-04-17\")], \"positive\"] = 1010\n",
    "    \n",
    "    data.loc[idx[\"Marche\", pd.Timestamp(\"2020-05-18\")], \"positive\"] = 6671\n",
    "    data.loc[idx[\"Marche\", pd.Timestamp(\"2020-07-18\")], \"positive\"] = 6811\n",
    "    \n",
    "    data.loc[idx[\"Basilicata\", pd.Timestamp(\"2020-05-03\")], \"positive\"] = 380\n",
    "    data.loc[idx[\"Basilicata\", pd.Timestamp(\"2020-05-04\")], \"positive\"] = 380\n",
    "    data.loc[idx[\"Basilicata\", pd.Timestamp(\"2020-05-05\")], \"positive\"] = 380\n",
    "    data.loc[idx[\"Basilicata\", pd.Timestamp(\"2020-05-06\")], \"positive\"] = 380\n",
    "    data.loc[idx[\"Basilicata\", pd.Timestamp(\"2020-05-07\")], \"positive\"] = 380\n",
    "    data.loc[idx[\"Basilicata\", pd.Timestamp(\"2020-07-11\")], \"positive\"] = 405\n",
    "    data.loc[idx[\"Basilicata\", pd.Timestamp(\"2020-07-12\")], \"positive\"] = 405\n",
    "    data.loc[idx[\"Basilicata\", pd.Timestamp(\"2020-07-13\")], \"positive\"] = 405\n",
    "    data.loc[idx[\"Basilicata\", pd.Timestamp(\"2020-07-14\")], \"positive\"] = 405\n",
    "    data.loc[idx[\"Basilicata\", pd.Timestamp(\"2020-08-15\")], \"positive\"] = 485\n",
    "    \n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-05-03\")], \"positive\"] = 1315\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-05-20\")], \"positive\"] = 1354\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-05-21\")], \"positive\"] = 1354\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-05-22\")], \"positive\"] = 1354\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-05-23\")], \"positive\"] = 1354\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-05-24\")], \"positive\"] = 1354\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-07-27\")], \"positive\"] = 1388\n",
    "    \n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-12\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-13\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-14\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-15\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-16\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-17\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-18\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-19\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-20\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-21\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-22\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-23\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-24\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-25\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-26\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-27\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-28\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-29\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-30\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-05-31\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-06-01\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-06-02\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-06-03\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-06-04\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-06-05\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-06-06\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-06-07\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-06-08\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-06-09\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-06-10\")], \"positive\"] = 4608\n",
    "    data.loc[idx[\"Campania\", pd.Timestamp(\"2020-06-11\")], \"positive\"] = 4608\n",
    "    \n",
    "    data.loc[idx[\"Emilia-Romagna\", pd.Timestamp(\"2020-03-28\")], \"total\"] = 48619\n",
    "    data.loc[idx[\"Emilia-Romagna\", pd.Timestamp(\"2020-03-29\")], \"total\"] = 49439\n",
    "    \n",
    "    data.loc[idx[\"Friuli Venezia Giulia\", pd.Timestamp(\"2020-03-19\")], \"total\"] = 4958\n",
    "    \n",
    "    data.loc[idx[\"Lombardia\", pd.Timestamp(\"2020-02-25\")], \"total\"] = 2336\n",
    "    \n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-04-27\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-04-28\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-04-29\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-04-30\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-01\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-02\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-03\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-04\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-05\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-06\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-07\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-08\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-09\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-10\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-11\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-12\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-13\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-14\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-15\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-16\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-17\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-18\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-19\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-20\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-21\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-22\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-23\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-24\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-25\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-26\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-27\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-28\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-29\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-30\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-05-31\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-06-01\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-06-02\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-06-03\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-06-04\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-06-05\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-06-06\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-06-07\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-06-08\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-06-09\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-06-10\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-06-11\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-06-12\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-06-13\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-06-14\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-06-15\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-06-16\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-06-17\")], \"positive\"] = 3070\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-06-18\")], \"positive\"] = 3070\n",
    "    \n",
    "    data.loc[idx[\"Liguria\", pd.Timestamp(\"2020-02-29\")], \"positive\"] = 20\n",
    "    data.loc[idx[\"Liguria\", pd.Timestamp(\"2020-03-01\")], \"positive\"] = 21\n",
    "    \n",
    "    data.loc[idx[\"P.A. Bolzano\", pd.Timestamp(\"2020-06-14\")], \"positive\"] = 2610\n",
    "    data.loc[idx[\"P.A. Bolzano\", pd.Timestamp(\"2020-07-28\")], \"positive\"] = 2700\n",
    "    data.loc[idx[\"P.A. Bolzano\", pd.Timestamp(\"2020-07-29\")], \"positive\"] = 2700\n",
    "    \n",
    "    data.loc[idx[\"Piemonte\", pd.Timestamp(\"2020-02-27\")], \"positive\"] = 3\n",
    "    data.loc[idx[\"Piemonte\", pd.Timestamp(\"2020-03-09\")], \"positive\"] = 360\n",
    "    \n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-06-06\")], \"positive\"] = 1360\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-06-07\")], \"positive\"] = 1360\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-06-08\")], \"positive\"] = 1360\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-06-09\")], \"positive\"] = 1360\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-06-10\")], \"positive\"] = 1360\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-06-11\")], \"positive\"] = 1360\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-06-12\")], \"positive\"] = 1360\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-06-13\")], \"positive\"] = 1360\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-06-14\")], \"positive\"] = 1360\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-06-15\")], \"positive\"] = 1360\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-06-16\")], \"positive\"] = 1360\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-06-17\")], \"positive\"] = 1360\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-06-18\")], \"positive\"] = 1360\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-06-19\")], \"positive\"] = 1360\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-06-20\")], \"positive\"] = 1360\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-06-21\")], \"positive\"] = 1360\n",
    "    data.loc[idx[\"Sardegna\", pd.Timestamp(\"2020-06-22\")], \"positive\"] = 1360\n",
    "    \n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-03-02\")], \"positive\"] = 9\n",
    "    data.loc[idx[\"Sicilia\", pd.Timestamp(\"2020-03-03\")], \"positive\"] = 9\n",
    "    \n",
    "    data.loc[idx[\"P.A. Trento\", pd.Timestamp(\"2020-06-24\") :], \"positive\"] -= 385\n",
    "    \n",
    "    data.loc[idx[\"Puglia\", pd.Timestamp(\"2020-06-26\")], \"positive\"] = 4530\n",
    "    data.loc[idx[\"Puglia\", pd.Timestamp(\"2020-06-27\")], \"positive\"] = 4530\n",
    "    data.loc[idx[\"Puglia\", pd.Timestamp(\"2020-06-28\")], \"positive\"] = 4530\n",
    "    data.loc[idx[\"Puglia\", pd.Timestamp(\"2020-06-29\")], \"positive\"] = 4530\n",
    "    data.loc[idx[\"Puglia\", pd.Timestamp(\"2020-06-30\")], \"positive\"] = 4530\n",
    "    data.loc[idx[\"Puglia\", pd.Timestamp(\"2020-07-07\")], \"positive\"] = 4536\n",
    "    data.loc[idx[\"Puglia\", pd.Timestamp(\"2020-07-19\")], \"positive\"] = 4556\n",
    "    data.loc[idx[\"Puglia\", pd.Timestamp(\"2020-07-20\")], \"positive\"] = 4556\n",
    "    \n",
    "    data.loc[idx[\"Valle d'Aosta\", pd.Timestamp(\"2020-03-14\")], \"total\"] = 230\n",
    "    data.loc[idx[\"Valle d'Aosta\", pd.Timestamp(\"2020-06-22\")], \"total\"] = 17491\n",
    "    \n",
    "    date_index = data.index.get_level_values(1).unique()\n",
    "    \n",
    "    # Add - Trentino Alto Adige\n",
    "    trentino_alto_adige_index = pd.MultiIndex.from_product([[\"Trentino Alto Adige\"], date_index], names=[\"region\", \"date\"])\n",
    "    trentino_alto_adige = pd.DataFrame(index=trentino_alto_adige_index).sort_index()\n",
    "    trentino_alto_adige[\"positive\"] = data.loc[\"P.A. Trento\"][\"positive\"].values + data.loc[\"P.A. Bolzano\"][\"positive\"].values\n",
    "    trentino_alto_adige[\"total\"] = data.loc[\"P.A. Trento\"][\"total\"].values + data.loc[\"P.A. Bolzano\"][\"total\"].values\n",
    "    data = data.append(trentino_alto_adige)\n",
    "    \n",
    "    # Add - Nord Italia\n",
    "    nord_italia_index = pd.MultiIndex.from_product([[\"Nord Italia\"], date_index], names=[\"region\", \"date\"])\n",
    "    nord_italia = pd.DataFrame(index=nord_italia_index).sort_index()\n",
    "    nord_italia[\"positive\"] = data.loc[\"Valle d\\'Aosta\"][\"positive\"].values + data.loc[\"Piemonte\"][\"positive\"].values + data.loc[\"Liguria\"][\"positive\"].values + data.loc[\"Lombardia\"][\"positive\"].values + data.loc[\"P.A. Trento\"][\"positive\"].values + data.loc[\"P.A. Bolzano\"][\"positive\"].values + data.loc[\"Veneto\"][\"positive\"].values + data.loc[\"Friuli Venezia Giulia\"][\"positive\"].values + data.loc[\"Emilia-Romagna\"][\"positive\"].values\n",
    "    nord_italia[\"total\"] = data.loc[\"Valle d\\'Aosta\"][\"total\"].values + data.loc[\"Piemonte\"][\"total\"].values + data.loc[\"Liguria\"][\"total\"].values + data.loc[\"Lombardia\"][\"total\"].values + data.loc[\"P.A. Trento\"][\"total\"].values + data.loc[\"P.A. Bolzano\"][\"total\"].values + data.loc[\"Veneto\"][\"total\"].values + data.loc[\"Friuli Venezia Giulia\"][\"total\"].values + data.loc[\"Emilia-Romagna\"][\"total\"].values\n",
    "    data = data.append(nord_italia)\n",
    "    \n",
    "    # Add - Centro Italia\n",
    "    centro_italia_index = pd.MultiIndex.from_product([[\"Centro Italia\"], date_index], names=[\"region\", \"date\"])\n",
    "    centro_italia = pd.DataFrame(index=centro_italia_index).sort_index()\n",
    "    centro_italia[\"positive\"] = data.loc[\"Toscana\"][\"positive\"].values + data.loc[\"Marche\"][\"positive\"].values + data.loc[\"Umbria\"][\"positive\"].values + data.loc[\"Lazio\"][\"positive\"].values\n",
    "    centro_italia[\"total\"] = data.loc[\"Toscana\"][\"total\"].values + data.loc[\"Marche\"][\"total\"].values + data.loc[\"Umbria\"][\"total\"].values + data.loc[\"Lazio\"][\"total\"].values\n",
    "    data = data.append(centro_italia)\n",
    "    \n",
    "    # Add - Sud Italia\n",
    "    sud_italia_index = pd.MultiIndex.from_product([[\"Sud Italia\"], date_index], names=[\"region\", \"date\"])\n",
    "    sud_italia = pd.DataFrame(index=sud_italia_index).sort_index()\n",
    "    sud_italia[\"positive\"] = data.loc[\"Abruzzo\"][\"positive\"].values + data.loc[\"Molise\"][\"positive\"].values + data.loc[\"Campania\"][\"positive\"].values + data.loc[\"Basilicata\"][\"positive\"].values + data.loc[\"Puglia\"][\"positive\"].values + data.loc[\"Calabria\"][\"positive\"].values + data.loc[\"Sicilia\"][\"positive\"].values + data.loc[\"Sardegna\"][\"positive\"].values\n",
    "    sud_italia[\"total\"] = data.loc[\"Abruzzo\"][\"total\"].values + data.loc[\"Molise\"][\"total\"].values + data.loc[\"Campania\"][\"total\"].values + data.loc[\"Basilicata\"][\"total\"].values + data.loc[\"Puglia\"][\"total\"].values + data.loc[\"Calabria\"][\"total\"].values + data.loc[\"Sicilia\"][\"total\"].values + data.loc[\"Sardegna\"][\"total\"].values\n",
    "    data = data.append(sud_italia)\n",
    "    \n",
    "    # Add - Italia\n",
    "    italia_index = pd.MultiIndex.from_product([[\"Italia\"], date_index], names=[\"region\", \"date\"])\n",
    "    italia = pd.DataFrame(index=italia_index).sort_index()\n",
    "    italia[\"positive\"] = data.loc[\"Nord Italia\"][\"positive\"].values + data.loc[\"Centro Italia\"][\"positive\"].values + data.loc[\"Sud Italia\"][\"positive\"].values\n",
    "    italia[\"total\"] = data.loc[\"Nord Italia\"][\"total\"].values + data.loc[\"Centro Italia\"][\"total\"].values + data.loc[\"Sud Italia\"][\"total\"].values\n",
    "    data = data.append(italia)\n",
    "    \n",
    "    all_cases = data['positive']\n",
    "    data = data.diff().fillna(0).clip(0, None).sort_index()\n",
    "    data['all_cases'] = all_cases\n",
    "    \n",
    "    return data.loc[idx[:, :run_date], [\"positive\", \"total\", \"all_cases\"]]\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_and_process_covid_data_it(run_date: pd.Timestamp):\n",
    "    \"\"\" Helper function for getting and processing COVIDTracking data at once \"\"\"\n",
    "    data = get_raw_covid_data_it()\n",
    "    data = process_covid_data_it(data, run_date)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "import typing\n",
    "import pandas as pd\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Data loading functions for different countries may be registered here.\n",
    "# For US, the data loader is pre-registered. Additional countries may be\n",
    "# registered upon import of third-party modules.\n",
    "# Data cleaning must be done by the data loader function!\n",
    "LOADERS:typing.Dict[str, typing.Callable[[pd.Timestamp], pd.DataFrame]] = {\n",
    "    'it': get_and_process_covid_data_it\n",
    "}\n",
    "\n",
    "\n",
    "def get_data(country: str, run_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    \"\"\" Retrieves data for a country using the registered data loader method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    country : str\n",
    "        short code of the country (key in LOADERS dict)\n",
    "    run_date : pd.Timestamp\n",
    "        date when the analysis is performed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model_input : pd.DataFrame\n",
    "        Data as returned by data loader function.\n",
    "        Ideally \"as it was on `run_date`\", meaning that information such as corrections\n",
    "        that became available after `run_date` should not be taken into account.\n",
    "        This is important to realistically back-test how the model would have performed at `run_date`.\n",
    "    \"\"\"\n",
    "    if not country in LOADERS:\n",
    "        raise KeyError(f\"No data loader for '{country}' is registered.\")\n",
    "    result = LOADERS[country](run_date)\n",
    "    assert isinstance(result, pd.DataFrame)\n",
    "    assert result.index.names == (\"region\", \"date\")\n",
    "    assert \"positive\" in result.columns\n",
    "    assert \"total\" in result.columns\n",
    "    return result\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "\n",
    "from scipy import stats as sps\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "class bettencourt_ribeiro():\n",
    "    def __init__(self, data, regions):\n",
    "        self.data=data\n",
    "        self.regions=regions\n",
    "        self.R_T_MAX = 12\n",
    "        self.r_t_range = np.linspace(0, self.R_T_MAX, self.R_T_MAX*100+1)\n",
    "        self.GAMMA = 1/6.6\n",
    "        self.ROLLING_DAYS = 7\n",
    "        \n",
    "\n",
    "    def highest_density_interval(self, pmf, p=.9, debug=False):\n",
    "        if(isinstance(pmf, pd.DataFrame)):\n",
    "            return pd.DataFrame([self.highest_density_interval(pmf[col], p=p) for col in pmf],\n",
    "                                index=pmf.columns)\n",
    "\n",
    "        cumsum = np.cumsum(pmf.values)\n",
    "\n",
    "        # N x N matrix of total probability mass for each low, high\n",
    "        total_p = cumsum - cumsum[:, None]\n",
    "\n",
    "        # Return all indices with total_p > p\n",
    "\n",
    "        remove_nan = lambda x: np.nan_to_num(x)\n",
    "        total_p = remove_nan(total_p)\n",
    "        lows, highs = (total_p > p).nonzero()\n",
    "\n",
    "        # Find the smallest range (highest density)\n",
    "        low, high = 0, 0\n",
    "        if(len(highs) > 0 and len(lows) > 0):\n",
    "            best = (highs - lows).argmin()\n",
    "            low = pmf.index[lows[best]]\n",
    "            high = pmf.index[highs[best]]\n",
    "\n",
    "        return pd.Series([low, high], index=[f'Low_{p*100:.0f}', f'High_{p*100:.0f}'])\n",
    "\n",
    "\n",
    "    def prepare_cases(self, cases):\n",
    "        new_cases = cases.diff()\n",
    "\n",
    "        smoothed = new_cases.rolling(self.ROLLING_DAYS,\n",
    "            win_type='gaussian',\n",
    "            min_periods=1,\n",
    "            center=True).mean(std=3).round()\n",
    "\n",
    "        smoothed_tmp = smoothed\n",
    "        for i in range(10, 0, -1):\n",
    "            idx_start = np.searchsorted(smoothed, i)\n",
    "            smoothed_tmp = smoothed.iloc[idx_start:]\n",
    "            original = new_cases.loc[smoothed_tmp.index]\n",
    "            if len(smoothed_tmp) > 0:\n",
    "                break\n",
    "\n",
    "        return original, smoothed_tmp\n",
    "    \n",
    "    \n",
    "    def get_posteriors(self, sr, sigma=0.15):\n",
    "        # (1) Calculate Lambda\n",
    "        lam = sr[:-1].values * np.exp(self.GAMMA * (self.r_t_range[:, None] - 1))\n",
    "\n",
    "        # (2) Calculate each day's likelihood\n",
    "        likelihoods = pd.DataFrame(\n",
    "            data = sps.poisson.pmf(sr[1:].values, lam),\n",
    "            index = self.r_t_range,\n",
    "            columns = sr.index[1:])\n",
    "\n",
    "        # (3) Create the Gaussian Matrix\n",
    "        process_matrix = sps.norm(loc=self.r_t_range,\n",
    "                                  scale=sigma\n",
    "                                 ).pdf(self.r_t_range[:, None]) \n",
    "\n",
    "        # (3a) Normalize all rows to sum to 1\n",
    "        process_matrix /= process_matrix.sum(axis=0)\n",
    "\n",
    "        # (4) Calculate the initial prior\n",
    "        prior0 = np.ones_like(self.r_t_range)/len(self.r_t_range)\n",
    "        prior0 /= prior0.sum()\n",
    "\n",
    "        # Create a DataFrame that will hold our posteriors for each day\n",
    "        # Insert our prior as the first posterior.\n",
    "        posteriors = pd.DataFrame(\n",
    "            index=self.r_t_range,\n",
    "            columns=sr.index,\n",
    "            data={sr.index[0]: prior0}\n",
    "        )\n",
    "\n",
    "        # Track of the sum of the log of the probability\n",
    "        # of the data for maximum likelihood calculation.\n",
    "        log_likelihood = 0.0\n",
    "\n",
    "        # (5) Iteratively apply Bayes' rule\n",
    "        for previous_day, current_day in zip(sr.index[:-1], sr.index[1:]):\n",
    "\n",
    "            #(5a) Calculate the new prior\n",
    "            current_prior = process_matrix @ posteriors[previous_day]\n",
    "\n",
    "            #(5b) Calculate the numerator of Bayes' Rule: P(k|R_t)P(R_t)\n",
    "            numerator = likelihoods[current_day] * current_prior\n",
    "\n",
    "            #(5c) Calcluate the denominator of Bayes' Rule P(k)\n",
    "            denominator = np.sum(numerator)\n",
    "            denominator = 0.000001 if denominator <= 0 else denominator\n",
    "\n",
    "            # Execute full Bayes' Rule\n",
    "            posteriors[current_day] = numerator/denominator\n",
    "\n",
    "            # Add to the running sum of log likelihoods\n",
    "            log_likelihood += np.log(denominator)\n",
    "\n",
    "        return posteriors, log_likelihood\n",
    "        \n",
    "        \n",
    "    def run_model(self, region_name):\n",
    "        sigmas = np.linspace(1/20, 1, 20)\n",
    "        cases = self.data['all_cases'][region_name].astype(int)\n",
    "        new, smoothed = self.prepare_cases(cases)\n",
    "        if len(smoothed) == 0:\n",
    "            new, smoothed = self.prepare_cases(cases)\n",
    "        result = {}\n",
    "        # Holds all posteriors with every given value of sigma\n",
    "        result['posteriors'] = []\n",
    "        # Holds the log likelihood across all k for each value of sigma\n",
    "        result['log_likelihoods'] = []\n",
    "\n",
    "        for sigma in sigmas:\n",
    "            posteriors, log_likelihood = self.get_posteriors(smoothed, sigma=sigma)\n",
    "            result['posteriors'].append(posteriors)\n",
    "            result['log_likelihoods'].append(log_likelihood)\n",
    "        \n",
    "        print(region_name)\n",
    "\n",
    "        return result, region_name\n",
    "    \n",
    "    \n",
    "    def run(self):\n",
    "        sigmas = np.linspace(1/20, 1, 20)\n",
    "        results = {}\n",
    "        \n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            res = executor.map(self.run_model, self.regions)\n",
    "            for r in list(res):\n",
    "                # Store all results keyed off of state name\n",
    "                results[r[1]] = r[0]\n",
    "\n",
    "        # Each index of this array holds the total of the log likelihoods for\n",
    "        # the corresponding index of the sigmas array.\n",
    "        total_log_likelihoods = np.zeros_like(sigmas)\n",
    "\n",
    "        # Loop through each state's results and add the log likelihoods to the running total.\n",
    "        for region_name, result in results.items():\n",
    "            total_log_likelihoods += result['log_likelihoods']\n",
    "\n",
    "        # Select the index with the largest log likelihood total\n",
    "        max_likelihood_index = total_log_likelihoods.argmax()\n",
    "\n",
    "        final_results = None\n",
    "\n",
    "        for region_name, result in results.items():\n",
    "            posteriors = result['posteriors'][max_likelihood_index]\n",
    "            hdis_95 = self.highest_density_interval(posteriors, p=.95)\n",
    "            hdis_90 = self.highest_density_interval(posteriors, p=.90)\n",
    "            hdis_68 = self.highest_density_interval(posteriors, p=.68)\n",
    "            hdis_50 = self.highest_density_interval(posteriors, p=.50)\n",
    "            most_likely = posteriors.idxmax().rename('ML')\n",
    "            result = pd.concat([most_likely, hdis_95, hdis_90, hdis_68, hdis_50], axis=1)\n",
    "            result.insert(0, 'region', region_name)\n",
    "            result.insert(1, 'date', most_likely.index)\n",
    "            if final_results is None:\n",
    "                final_results = result\n",
    "            else:\n",
    "                final_results = pd.concat([final_results, result])\n",
    "                \n",
    "        final_results = final_results.groupby('region').apply(lambda x: x.iloc[1:])\n",
    "        return final_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = get_data(country='it', run_date=pd.Timestamp.today()) #Non funzionas un cazzo\n",
    "regions = [data.index.get_level_values(0).unique()[0],data.index.get_level_values(0).unique()[1]]\n",
    "\n",
    "total_execution_time = time.time()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "br = bettencourt_ribeiro(data, regions)\n",
    "output_bettencourt_ribeiro = br.run()\n",
    "output_bettencourt_ribeiro.to_csv('provaremoto.csv', index=False)\n",
    "\n",
    "print(\"\\n- Executed in: %.2f seconds\" % (time.time() - start_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ddace4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
